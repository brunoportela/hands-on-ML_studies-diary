{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\n",
    "What is the fundamental idea behind Support Vector Machines?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental idea is to fit the widest \"street\" between the classes, which means separating the two classes and training instances as far as possible.<br>\n",
    "We can use the soft margin classification, which looks for a trade off on training instances separation and classes separation. In this case, a few training instances my end up on the other side of the \"road\" (which could be only outliers in some cases), but it might result in a wider street separating both classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "What is support vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are instances located on the edge of the street. There instances will determine the decision boundaries of the model. If many instances are added \"off street\", this won't affect the decision boundary. On instances inserted on road might affect how the boundaries are determined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "Why is it important to scale the inputs when using SVMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nature of a SVM is to fit the \"largest road possible\". If the set is not scaled correctly, SVM will ignore small features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\n",
    "Can an SVM classifier output a confidence score when it classifies an instance? What about a probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It outputs the confidence score, but unlike logistic regression, it doesn't output probabilities for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete answer from the book:<br>\n",
    "An SVM classifier can output the distance between the test instance and the decision boundary, and you can use this as a confidence score. However, this score cannot be directly converted into an estimation of the class probability. If you set probability = True when creating an SVM in Scikit-Learn, then after training it will calibrate the probabilities using Logistic Regression on the SVM’s scores (trained by an additional five-fold cross-validation on the training data). This will add the predict_proba( ) and predict_log_proba( ) methods to the SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\n",
    "Should you use the primal or the dual form of the SVM problem to train a model on a training set with millions of instances and hundreds of features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primal. Dual is faster to solve when the number of training instances is smaller than the number of features, which is NOT the case here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.\n",
    "Say you’ve trained an SVM classifier with an RBF kernel, but it seems to underfit the training set. Should you increase or decrease γ (gamma)? What about C?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should increase the gamma or C value (or maybe both). They act like regularization hyperparamenters, so:\n",
    "- when overfitting, decrease them;\n",
    "- when underfitting, increase them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.\n",
    "How should you set the QP parameters (H, f, A, and b) to solve the soft margin linear SVM classifier problem using an off-the-shelf QP solver?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is similar to QP solver for hard margins. The difference is that you would need to have m additional paramenters and m additional constraints.\n",
    "\n",
    "Refer to book, pages 167 and 168; and appendix A, page 724."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 9. 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find these questions on different notebooks, in the same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
