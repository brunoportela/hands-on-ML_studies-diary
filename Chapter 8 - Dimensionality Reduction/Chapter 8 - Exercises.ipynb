{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\n",
    "What are the main motivations for reducing a dataset’s dimensionality? What are the main drawbacks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speed up training, reduce dataset byte size and help in visualizing the data and gaining insight on important features.<br>\n",
    "The drawback is that it can lose important data, can be extremely computional intensive, add complexity to the pipeline and some transformed features might become hard to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "What is the curse of dimensionality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly sampled high dimensional vectors are usually very sparse, making is very easy to overfit the data. To avoid this, it is required to have a huge number of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "Once a dataset’s dimensionality has been reduced, is it possible to reverse the operation? If so, how? If not, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, because some data is almost always lost during the compression. PCA has a simple type of reduction process and we can give it a shot to revert it back, but it is extremely unlikely that the conversion will be 100% the same as the original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\n",
    "Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, as long as it has useless dimensions. If there are no useless dimensions, a reduction with PCA will lose too much information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\n",
    "Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained variance ratio to 95%. How many dimensions will the resulting dataset have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will depend on the dataset.\n",
    "\n",
    "For a dataset with all points perfectly aligned, it can be reduced to only one dimension; while on a dataset that all 1000 points are spreaded across all 1000 dimensions, then our best chance it to have a reduction to 950 dimensions (95% of 1000).\n",
    "\n",
    "We can plot the explained variance as a function of the number of dimensions as a way to get an estimate of the dataset's intrinsic dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.\n",
    "In what cases would you use vanilla PCA, Incremental PCA, Randomized PCA, or Kernel PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"Vanilla\" PCA: used when the dataset fits in memory;\n",
    "- Incremental PCA: used when the dataset does not fit in memory. It is slower than the regular PCA. Also useful for online tasks;\n",
    "- Randomized PCA: used when we want to reduce the dimensionality and the dataset fits in memory. It is faster than the regular PCA;\n",
    "- Kernel PCA: used for nonlinear datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.\n",
    "How can you evaluate the performance of a dimensionality reduction algorithm on your dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- when possible, apply reverse transformation and measure the reconstruction error;\n",
    "- if using dimensionality reduction as a preprocessing step before a machine learning algorithm, you can measure the performance of that second algorithm; if dimensionality didn't lose too much information, then the algorithm should perform just as well as wehn using the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.\n",
    "Does it make any sense to chain two different dimensionality reduction algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it can make sense.<br>\n",
    "A common approach is to use PCA as a first step, to get rid of lots of useless information, in a sort of superficial quick cleaning; as a second step, use a more intensive dimensionality reduction algorithm, such as LLE, which will do some fine cleaning.\n",
    "\n",
    "Most likely, the result is the same as using only LLE, however it will be completed way faster if done chained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. 10.\n",
    "Another notebook, same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
