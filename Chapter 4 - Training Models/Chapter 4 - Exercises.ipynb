{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\n",
    "Which Linear Regression training algorithm can you use if you have a training set with millions of features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent or Mini-Batch Gradient Descent.<br>\n",
    "Batch descent can also work, if those millions of features fit in the memory.<br>\n",
    "Normal equation and SVD should be avoided as the complexity cost would make it impractical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "Suppose the features in your training set have very different scales. Which algorithms might suffer from this, and how? What can you do about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent would suffer from features being on different scales. The cost function will have a shape of an elongated bowl, causing the the GD algorithm to take too long to reach the global minimum. To solve this, we can scale the features using StandardScaler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. The cost function is a convex - if you draw a straigh line between two points in the curve, it will never cross the curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\n",
    "Do all Gradient Descent algorithms lead to the same model, provided you let them run long enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not necessarily.<br>\n",
    "Assuming that the learning rate is not too high, linear regression or logistic regression (covex problems) will approach the global minimum and produce results very similar.<br>\n",
    "As for Stochastic GD and Mini-batch GD, the learning rate has to be continuously and gradually reduced so it can converge, otherwise it will not settle on the global optimum, even if you let it running for very long time, resulting in different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\n",
    "Suppose you use Batch Gradient Descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on? How can you fix this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate is too high, causing the algorithm to diverge (jumping from on side to the other of the curve, for example). I would need to reduce the learning rate.<br>\n",
    "If this is happening on the training set as well, it means that the model is overfitting and I should stop the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.\n",
    "Is it a good idea to stop Mini-batch Gradient Descent immediately when the validation error goes up?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. Due to its random nature, it is not expected to have a decrease on validation error at every step, as the reduction happens on the average.<br>\n",
    "What can be done is save the progress at regular intervals and determine a tolerance threshold (for example, 10). If after 10 consecutive attempts it didn't decrease the validation error, then we can stop the training and use the best saved model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.\n",
    "Which Gradient Descent algorithm (among those we discussed) will reach the vicinity of the optimal solution the fastest? Which will actually converge? How can you make the others converge as well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fastest: Stochastic GD\n",
    "- Actually converge: Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.\n",
    "Suppose you are using Polynomial Regression. You plot the learning curves and you notice that there is a large gap between the training error and the validation error. What is happening? What are three ways to solve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It means that the model is performing better on the training set, than on the validation set. In other words, classic example of overfitting.<br>\n",
    "To solve the problem, we could:\n",
    "- reduce the polynomial degree, giving less freedom to the model.\n",
    "- regularize the model with Ridge or Lasso.\n",
    "- get more training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.\n",
    "Suppose you are using Ridge Regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regularization hyperparameter α or reduce it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High bias. We should reduce the regularization hyperparameter α."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.\n",
    "Why would you want to use:\n",
    "\n",
    "a. Ridge Regression instead of plain Linear Regression (i.e., without any regularization)?\n",
    "\n",
    "b. Lasso instead of Ridge Regression?\n",
    "\n",
    "c. Elastic Net instead of Lasso?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers:\n",
    "\n",
    "a) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
