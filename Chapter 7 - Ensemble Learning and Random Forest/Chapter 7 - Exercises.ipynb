{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\n",
    "If you have trained five different models on the exact same training data, and they all achieve 95% precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes. We can use a voting ensemble. If the models are very different, the ensemble can get us even better results. Also, when training with different instances, our chances of improving our scores are higher (that's the point of bagging and pasting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "What is the difference between hard and soft voting classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard Voting: This classifier will count the number of votes of each class and, whichever gets the most votes, is the chosen one.\n",
    "\n",
    "Soft Voting: This classifier will compute the average estimated class probability for each class, and picks the class with highest probability. This one usually performs better, as it gives more weights to high-confidence votes; however not all classifiers are able to estimate class probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, Random Forests, or stacking ensembles?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging, Pasting and Random Forest can be parallelized.<br>\n",
    "On the other hand, Boosting can't be parallelized, as they need to wait for the previous iteration result to adjust itself and run again.<br>\n",
    "Stacking is in between the two above mentioned regarding parallelization. Predictors in a given layer are independent, so they can run in parallel; however the next layer predictors can only start training after all previous layers' predictors have been trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\n",
    "What is the benefit of out-of-bag evaluation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During out of the bag evaluation, the training happens on only about 60-70% of the set. The remaider (with was held out) can be used to validate, instead of using the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\n",
    "What makes Extra-Trees more random than regular Random Forests? How can this extra randomness help? Are Extra-Trees slower or faster than regular Random Forests?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest needs to determine a splitting threshold value at each node. With Extra-Trees, the feature threshold is determined randomly, instead of having the need to search for the best possible value.<br>\n",
    "This works as an extra regularization. If the model overfits the training data, this randomness can help and get us a better result.\n",
    "This speeds up the process, as determining this threshold is the most time consuming part of the growing tree.<br>\n",
    "When it comes to making predictions, it performs at the same speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.\n",
    "If your AdaBoost ensemble underfits the training data, which hyperparameters should you tweak and how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase: number of estimators (n_estimators) and learning rate<br>\n",
    "Decrease: regularization hyperparameters of the base estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.\n",
    "If your Gradient Boosting ensemble overfits the training set, should you increase or decrease the learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decrease.<br>\n",
    "Another thing you should try is implementing some sort of early stopping, to find the best number of estimators (if overfitting, there are probably too many)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.  9.\n",
    "Find these questions on different notebooks, in the same folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
